{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "from thop import profile\n",
    "import tracemalloc\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, window_size):\n",
    "        super(SingleHeadSelfAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.window_size = window_size\n",
    "\n",
    "        # Linear projection for queries, keys, and values\n",
    "        self.proj = nn.Linear(embed_size, embed_size, bias=False)\n",
    "        self.fc_out = nn.Linear(embed_size, embed_size)\n",
    "     \n",
    "        self.scale = torch.sqrt(torch.FloatTensor([embed_size]))\n",
    "        if torch.cuda.is_available():\n",
    "            self.scale = self.scale.cuda()\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        N = x.shape[0]\n",
    "        seq_len = x.shape[1]\n",
    "\n",
    "        # Project input to queries, keys, and values\n",
    "        queries = self.proj(x)\n",
    "        keys = self.proj(x)\n",
    "        values = self.proj(x)\n",
    "\n",
    "        out = torch.zeros_like(x)\n",
    "        for start in range(0, seq_len, self.window_size):\n",
    "            end = min(start + self.window_size, seq_len)\n",
    "            q_window = queries[:, start:end]\n",
    "            k_window = keys[:, start:end]\n",
    "            v_window = values[:, start:end]\n",
    "\n",
    "            # Scaled dot-product attention\n",
    "            # Dimensions of q_window, k_window, v_window: (N, window_size, embed_size)\n",
    "            # Compute attention scores\n",
    "            energy = torch.bmm(q_window, k_window.transpose(1, 2)) / self.scale\n",
    "            if mask is not None:\n",
    "                mask_window = mask[:, start:end]\n",
    "                energy = energy.masked_fill(mask_window == 0, float(\"-1e20\"))\n",
    "\n",
    "            attention = torch.softmax(energy, dim=2)\n",
    "            out_window = torch.bmm(attention, v_window)\n",
    "            out[:, start:end] = out_window\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "        return out\n",
    "    \n",
    "def profile_self_attention(seq_length, device=\"cpu\"):\n",
    "    # Set up parameters\n",
    "    batch_size = 1\n",
    "    embed_size = 128\n",
    "    window_size = 64\n",
    "    \n",
    "    # Create an instance of SelfAttention\n",
    "    self_attention = SingleHeadSelfAttention(embed_size, window_size)\n",
    "\n",
    "    # Create a random input tensor\n",
    "    x = torch.randn(batch_size, seq_length, embed_size)\n",
    "\n",
    "    # Move model and input to GPU if available\n",
    "    # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    self_attention = self_attention.to(device)\n",
    "    x = x.to(device)\n",
    "    \n",
    "    def compute_flops(seq_len, window_size):\n",
    "        return 2 * seq_len**2 * embed_size + 2 * seq_len * embed_size**2\n",
    "\n",
    "    flops = compute_flops(seq_length, window_size)\n",
    "\n",
    "    # Measure memory usage\n",
    "    if device == \"cpu\":\n",
    "        process = psutil.Process()\n",
    "        initial_memory = process.memory_info().rss / 1024**2  # Initial memory in MB\n",
    "        _ = self_attention(x)\n",
    "        final_memory = process.memory_info().rss / 1024**2  # Final memory in MB\n",
    "        memory_usage = final_memory - initial_memory  # Memory usage in MB\n",
    "    else:\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        _ = self_attention(x)\n",
    "        memory_usage = torch.cuda.max_memory_allocated() / 1024**2  # Convert to MB\n",
    "        \n",
    "    # Measure time\n",
    "    start_time = time.time()\n",
    "    _ = self_attention(x)\n",
    "    end_time = time.time()\n",
    "    wall_clock_time = end_time - start_time\n",
    "\n",
    "    return flops, memory_usage, wall_clock_time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_lengths = [10, 100, 1000, 10000, 100000, 1000000]\n",
    "flops_list, memory_list, time_list = [], [], []\n",
    "N = 50\n",
    "for i in range(N):\n",
    "    for length in input_lengths:\n",
    "        flops, memory, wall_time = profile_self_attention(length, device = \"cuda\")\n",
    "        flops_list.append(flops)\n",
    "        memory_list.append(memory)\n",
    "        time_list.append(wall_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "profiling = pd.DataFrame({\"input_length\":input_lengths*N,\n",
    "              \"flops\": flops_list,\n",
    "              \"mem\": memory_list,\n",
    "              \"time\": time_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Plotting\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(131)\n",
    "sns.lineplot(data = profiling, x = \"input_length\", y = \"flops\")\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Input Length')\n",
    "plt.ylabel('FLOPS')\n",
    "plt.title('Computational Complexity')\n",
    "\n",
    "plt.subplot(132)\n",
    "sns.lineplot(data = profiling, x = \"input_length\", y = \"mem\")\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Input Length')\n",
    "plt.ylabel('Memory Usage (MB)')\n",
    "plt.title('Memory Usage')\n",
    "\n",
    "plt.subplot(133)\n",
    "sns.lineplot(data = profiling, x = \"input_length\", y = \"time\")\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Input Length')\n",
    "plt.ylabel('Wall Clock Time (s)')\n",
    "plt.title('Wall Clock Time')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ssm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
