{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SciFact, a dataset of 1.4K expert-written scientific claims paired with evidence-containing abstracts, and annotated with labels and rationales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import pickle\n",
    "import numpy as np\n",
    "from elasticsearch import Elasticsearch\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the file path where your embeddings were saved\n",
    "claim = \"scifact_claim_embeddings.pkl\"\n",
    "evidence = \"scifact_evidence_embeddings.pkl\"\n",
    "\n",
    "# Step 1: Load the pickled embeddings for evidence and claim\n",
    "with open(evidence, \"rb\") as f:\n",
    "    evidence_embeddings = pickle.load(f)\n",
    "\n",
    "with open(claim, \"rb\") as f:\n",
    "    claim_embeddings = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformat embeddings into numpy array\n",
    "claim_doc = []\n",
    "claim_mat = []\n",
    "for doc, embeddings in claim_embeddings.items():\n",
    "    claim_doc.append(doc)\n",
    "    claim_mat.append(embeddings)\n",
    "claim_mat = np.array(claim_mat)\n",
    "\n",
    "\n",
    "evidence_doc = []\n",
    "evidence_mat = []\n",
    "for doc, embeddings in evidence_embeddings.items():\n",
    "    evidence_doc.append(doc)\n",
    "    evidence_mat.append(embeddings)\n",
    "evidence_mat = np.array(evidence_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check on embedding dimensions\n",
    "n_evi, d = np.shape(evidence_mat)\n",
    "print(f\"Evidence embedding dimension: {(n_evi, d)}\")\n",
    "n_claim, d = np.shape(claim_mat)\n",
    "print(f\"Claim embedding dimension: {(n_claim, d)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get relevant documents index for claims\n",
    "scifact_evidence = load_dataset(\"allenai/scifact\", \"corpus\")\n",
    "scifact_claims = load_dataset(\"allenai/scifact\", \"claims\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_id =[doc[0] for doc in claim_doc]\n",
    "evidence_doc_id = [str(i) for i in scifact_evidence[\"train\"]['doc_id']]\n",
    "golden_evidence_id = []\n",
    "\n",
    "for claim in scifact_claims['train']:\n",
    "    if claim[\"id\"] in query_id:\n",
    "        if claim[\"evidence_doc_id\"] == \"\":\n",
    "            golden_evidence_id.append([-1])\n",
    "        else:\n",
    "            golden_evidence_id.append([evidence_doc_id.index(claim[\"evidence_doc_id\"])])\n",
    "        query_id.remove(claim[\"id\"])\n",
    "golden_evidence_id = np.array(golden_evidence_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Reciprocal Rank @ K (MRR@K)\n",
    "def mrr_at_k(actual, predicted, k):\n",
    "    \"\"\"\n",
    "    Calculate Mean Reciprocal Rank at K (MRR@K).\n",
    "    \n",
    "    Args:\n",
    "    actual: List of lists containing actual relevant items for each query.\n",
    "    predicted: List of lists containing predicted items for each query.\n",
    "    k: The number of top predictions to consider.\n",
    "    \n",
    "    Returns:\n",
    "    float: The MRR@K score.\n",
    "    \"\"\"\n",
    "    reciprocal_ranks = []\n",
    "    \n",
    "    for act, pred in zip(actual, predicted):\n",
    "        # Find the rank of the first relevant item in top K predictions\n",
    "        for rank, item in enumerate(pred[:k], 1):\n",
    "            if item in act:\n",
    "                reciprocal_ranks.append(1.0 / rank)\n",
    "                break\n",
    "        else:\n",
    "            reciprocal_ranks.append(0.0)\n",
    "    \n",
    "    return np.mean(reciprocal_ranks)\n",
    "\n",
    "def mean_average_precision_at_k(actual, predicted, k):\n",
    "    ap_at_k = []\n",
    "\n",
    "    map_at_k = np.mean(ap_at_k)\n",
    "    \n",
    "    return map_at_k\n",
    "\n",
    "def average_precision_at_k(relevant_docs, retrieved_docs, k):\n",
    "    \"\"\"\n",
    "    Calculate Average Precision at K for a single query\n",
    "    \n",
    "    Args:\n",
    "    relevant_docs (list): Indices of relevant documents\n",
    "    retrieved_docs (list): Indices of retrieved documents, in order of retrieval\n",
    "    k (int): Number of top results to consider\n",
    "    \n",
    "    Returns:\n",
    "    float: Average Precision at K\n",
    "    \"\"\"\n",
    "    if -1 in relevant_docs:\n",
    "        return 0.0\n",
    "    \n",
    "    relevant_docs = set(relevant_docs)\n",
    "    retrieved_docs = retrieved_docs[:k]\n",
    "    \n",
    "    precision_sum = 0\n",
    "    num_relevant = 0\n",
    "    \n",
    "    for i, doc in enumerate(retrieved_docs, 1):\n",
    "        if doc in relevant_docs:\n",
    "            num_relevant += 1\n",
    "            precision_sum += num_relevant / i\n",
    "    \n",
    "    return precision_sum / min(len(relevant_docs), k)\n",
    "\n",
    "def mean_average_precision_at_k(queries_relevant_docs, queries_retrieved_docs, k):\n",
    "    \"\"\"\n",
    "    Calculate Mean Average Precision at K (MAP@K) for multiple queries\n",
    "    \n",
    "    Args:\n",
    "    queries_relevant_docs (list of lists): List of relevant document indices for each query\n",
    "    queries_retrieved_docs (list of lists): List of retrieved document indices for each query\n",
    "    k (int): Number of top results to consider\n",
    "    \n",
    "    Returns:\n",
    "    float: MAP@K score\n",
    "    \"\"\"\n",
    "    ap_scores = [\n",
    "        average_precision_at_k(relevant, retrieved, k)\n",
    "        for relevant, retrieved in zip(queries_relevant_docs, queries_retrieved_docs)\n",
    "    ]\n",
    "    return np.mean(ap_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Nearest Neighbor with FAISS\n",
    "reference: https://github.com/facebookresearch/faiss/wiki/Getting-started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build index with Evidence Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.IndexFlatL2(d)   # build the index\n",
    "print(index.is_trained)\n",
    "index.add(evidence_mat)  # add vectors to the index\n",
    "print(index.ntotal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 50\n",
    "D, I = index.search(claim_mat, K)     # actual search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of MAP and MRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_at_1 = mean_average_precision_at_k(golden_evidence_id, I, 1)\n",
    "map_at_10 = mean_average_precision_at_k(golden_evidence_id, I, 10)\n",
    "map_at_50 = mean_average_precision_at_k(golden_evidence_id, I, 50)\n",
    "print(f\"MAP@1: {map_at_1:.4f}\")\n",
    "print(f\"MAP@10: {map_at_10:.4f}\")\n",
    "print(f\"MAP@50: {map_at_50:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrr_at_1 = mrr_at_k(actual=golden_evidence_id, predicted=I, k=1)\n",
    "mrr_at_10 = mrr_at_k(actual=golden_evidence_id, predicted=I, k=10)\n",
    "mrr_at_50 = mrr_at_k(actual=golden_evidence_id, predicted=I, k=50)\n",
    "print(f\"MRR@1:{mrr_at_1}, MRR@10: {mrr_at_10}, MRR@50: {mrr_at_50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 ElasticSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Elasticsearch instance\n",
    "es = Elasticsearch(\"http://localhost:9200\")\n",
    "\n",
    "# Check if Elasticsearch is running\n",
    "if not es.ping():\n",
    "    raise ValueError(\"Connection failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Index with Evidence Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Index documents in Elasticsearch\n",
    "for i, doc in enumerate(evidence_doc):\n",
    "    es.index(index = \"evidence-index\", id = i, body ={\"text\": doc[1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I = []\n",
    "for claim in claim_doc:\n",
    "    query = claim[1]\n",
    "    response = es.search(index = \"evidence-index\", \n",
    "                     body ={\"query\": \n",
    "                            {\"match\": \n",
    "                             {\"text\": query}\n",
    "                             }\n",
    "                             ,\n",
    "                             \"size\": 50})\n",
    "    topk = [int(hit['_id']) for hit in response[\"hits\"][\"hits\"]]\n",
    "    I.append(topk)\n",
    "I = np.array(I)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrr_at_1 = mrr_at_k(actual=golden_evidence_id, predicted=I, k=1)\n",
    "mrr_at_10 = mrr_at_k(actual=golden_evidence_id, predicted=I, k=10)\n",
    "mrr_at_50 = mrr_at_k(actual=golden_evidence_id, predicted=I, k=50)\n",
    "print(f\"MRR@1:{mrr_at_1}, MRR@10: {mrr_at_10}, MRR@50: {mrr_at_50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_at_1 = mean_average_precision_at_k(golden_evidence_id, I, 1)\n",
    "map_at_10 = mean_average_precision_at_k(golden_evidence_id, I, 10)\n",
    "map_at_50 = mean_average_precision_at_k(golden_evidence_id, I, 50)\n",
    "print(f\"MAP@1: {map_at_1:.4f}\")\n",
    "print(f\"MAP@10: {map_at_10:.4f}\")\n",
    "print(f\"MAP@50: {map_at_50:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ssm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
